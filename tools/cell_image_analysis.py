# tools/cell_image_analysis.py
"""
ç»†èƒæ ¸å›¾åƒåˆ†æå·¥å…·

ä½¿ç”¨é¢„è®­ç»ƒçš„ Unet æ¨¡å‹æ£€æµ‹å’Œè®¡æ•°æ˜¾å¾®å›¾åƒä¸­çš„ç»†èƒæ ¸ã€‚
å¤ç”¨ nuclei-segmentation-unet é¡¹ç›®çš„æ¨¡å‹æƒé‡ã€‚
"""
import os
import cv2
import numpy as np
import torch
import torch.nn as nn
from scipy import ndimage
from langchain_core.tools import tool
from config.settings import UNET_WEIGHTS_PATH


# ===== U-Net æ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰=====
class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.conv(x)


class UNet(nn.Module):
    def __init__(self, n_channels=1, n_classes=1):
        super(UNet, self).__init__()
        self.down1 = DoubleConv(n_channels, 64)
        self.down2 = DoubleConv(64, 128)
        self.down3 = DoubleConv(128, 256)
        self.down4 = DoubleConv(256, 512)
        self.pool = nn.MaxPool2d(2)

        self.bottleneck = DoubleConv(512, 1024)

        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)
        self.conv1 = DoubleConv(1024, 512)
        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)
        self.conv2 = DoubleConv(512, 256)
        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)
        self.conv3 = DoubleConv(256, 128)
        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
        self.conv4 = DoubleConv(128, 64)

        self.out = nn.Conv2d(64, n_classes, kernel_size=1)

    def forward(self, x):
        d1 = self.down1(x)
        p1 = self.pool(d1)
        d2 = self.down2(p1)
        p2 = self.pool(d2)
        d3 = self.down3(p2)
        p3 = self.pool(d3)
        d4 = self.down4(p3)
        p4 = self.pool(d4)

        b = self.bottleneck(p4)

        u1 = self.up1(b)
        c1 = self.conv1(torch.cat([d4, u1], dim=1))
        u2 = self.up2(c1)
        c2 = self.conv2(torch.cat([d3, u2], dim=1))
        u3 = self.up3(c2)
        c3 = self.conv3(torch.cat([d2, u3], dim=1))
        u4 = self.up4(c3)
        c4 = self.conv4(torch.cat([d1, u4], dim=1))

        return self.out(c4)


# ===== é¢„å¤„ç†å‡½æ•°ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰=====
def bio_preprocess(img):
    """ä¸­å€¼æ»¤æ³¢ + CLAHE å¯¹æ¯”åº¦å¢å¼º"""
    blurred = cv2.medianBlur(img, 5)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(blurred)
    return enhanced


# ===== æ¨¡å‹å•ä¾‹ç¼“å­˜ï¼ˆé¿å…æ¯æ¬¡è°ƒç”¨éƒ½é‡æ–°åŠ è½½ï¼‰=====
_model = None


def _get_model():
    """åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰"""
    global _model
    if _model is not None:
        return _model

    if not os.path.exists(UNET_WEIGHTS_PATH):
        raise FileNotFoundError(f"æ¨¡å‹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {UNET_WEIGHTS_PATH}")

    device = torch.device("cpu")
    _model = UNet(n_channels=1, n_classes=1).to(device)
    _model.load_state_dict(
        torch.load(UNET_WEIGHTS_PATH, map_location=device, weights_only=True)
    )
    _model.eval()
    print(f"[Unet] æ¨¡å‹åŠ è½½æˆåŠŸ: {UNET_WEIGHTS_PATH}")
    return _model


@tool
def cell_image_analysis(image_path: str) -> str:
    """
    åˆ†ææ˜¾å¾®å›¾åƒä¸­çš„ç»†èƒæ ¸ï¼Œè¿”å›æ£€æµ‹æ•°é‡å’Œåˆ†æç»“æœã€‚

    å½“ç”¨æˆ·éœ€è¦åˆ†æç»†èƒæ˜¾å¾®å›¾åƒã€æ£€æµ‹ç»†èƒæ ¸æ•°é‡ã€
    æˆ–è¯„ä¼°ç»†èƒå¯†åº¦æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚

    Args:
        image_path: ç»†èƒæ˜¾å¾®å›¾åƒçš„æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ .png, .jpg, .tif æ ¼å¼ï¼‰

    Returns:
        åŒ…å«ç»†èƒæ ¸æ•°é‡ã€é¢ç§¯ç»Ÿè®¡ç­‰ä¿¡æ¯çš„åˆ†ææŠ¥å‘Š
    """
    # === 1. éªŒè¯è¾“å…¥ ===
    if not os.path.exists(image_path):
        return f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°å›¾åƒæ–‡ä»¶ '{image_path}'ã€‚è¯·æä¾›æ­£ç¡®çš„æ–‡ä»¶è·¯å¾„ã€‚"

    valid_extensions = (".png", ".jpg", ".jpeg", ".tif", ".tiff")
    if not image_path.lower().endswith(valid_extensions):
        return f"é”™è¯¯ï¼šä¸æ”¯æŒçš„å›¾åƒæ ¼å¼ã€‚è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š{valid_extensions}"

    try:
        # === 2. è¯»å–å›¾åƒï¼ˆç°åº¦ï¼‰===
        original = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        if original is None:
            return f"é”™è¯¯ï¼šæ— æ³•è¯»å–å›¾åƒæ–‡ä»¶ '{image_path}'ã€‚æ–‡ä»¶å¯èƒ½å·²æŸåã€‚"

        original_h, original_w = original.shape

        # === 3. é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰===
        processed = bio_preprocess(original)
        processed_resized = cv2.resize(processed, (256, 256))

        # è½¬ä¸º tensor: (1, 1, 256, 256)
        img_tensor = torch.from_numpy(
            processed_resized / 255.0
        ).float().unsqueeze(0).unsqueeze(0)

        # === 4. æ¨¡å‹æ¨ç† ===
        model = _get_model()
        with torch.no_grad():
            output = model(img_tensor)
            pred_prob = torch.sigmoid(output).squeeze().cpu().numpy()
            binary_mask = (pred_prob > 0.5).astype(np.uint8)

        # === 5. è¿é€šåŸŸåˆ†æè®¡æ•° ===
        labeled_array, num_features = ndimage.label(binary_mask)

        # ç»Ÿè®¡æ¯ä¸ªç»†èƒæ ¸çš„é¢ç§¯ï¼Œè¿‡æ»¤å™ªç‚¹
        areas = []
        min_area = 10  # é¢ç§¯å°äº10åƒç´ çš„è§†ä¸ºå™ªç‚¹
        for i in range(1, num_features + 1):
            area = np.sum(labeled_array == i)
            if area >= min_area:
                areas.append(area)

        valid_count = len(areas)

        # === 6. ç”Ÿæˆåˆ†ææŠ¥å‘Š ===
        report_lines = [
            "ğŸ“Š ç»†èƒæ ¸å›¾åƒåˆ†ææŠ¥å‘Š",
            "=" * 40,
            f"å›¾åƒè·¯å¾„: {image_path}",
            f"åŸå§‹å°ºå¯¸: {original_w} Ã— {original_h} pixels",
            f"åˆ†æå°ºå¯¸: 256 Ã— 256 pixels",
            "",
            "ğŸ“Œ æ£€æµ‹ç»“æœ:",
            f"  æ£€æµ‹åˆ°ç»†èƒæ ¸: {valid_count} ä¸ª",
            f"  (å·²è¿‡æ»¤é¢ç§¯ < {min_area} åƒç´ çš„å™ªç‚¹)",
        ]

        if areas:
            report_lines.extend([
                "",
                "ğŸ“ é¢ç§¯ç»Ÿè®¡:",
                f"  å¹³å‡é¢ç§¯: {np.mean(areas):.1f} pixels",
                f"  æœ€å¤§é¢ç§¯: {max(areas)} pixels",
                f"  æœ€å°é¢ç§¯: {min(areas)} pixels",
                f"  é¢ç§¯æ ‡å‡†å·®: {np.std(areas):.1f} pixels",
                "",
                "ğŸ“ˆ å¯†åº¦è¯„ä¼°:",
                f"  æ©è†œè¦†ç›–ç‡: {np.sum(binary_mask) / binary_mask.size * 100:.2f}%",
                f"  ç»†èƒå¯†åº¦: {valid_count / (256 * 256) * 10000:.2f} ä¸ª/ä¸‡åƒç´ ",
            ])

        # === 7. ä¿å­˜é¢„æµ‹ç»“æœå›¾ ===
        output_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
            "outputs"
        )
        os.makedirs(output_dir, exist_ok=True)

        original_resized = cv2.resize(original, (256, 256))
        pred_visual = (binary_mask * 255).astype(np.uint8)
        combined = np.hstack([original_resized, pred_visual])

        save_name = f"analysis_{os.path.basename(image_path)}"
        save_path = os.path.join(output_dir, save_name)
        cv2.imwrite(save_path, combined)

        report_lines.extend([
            "",
            f"ğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜: {save_path}",
            "  (å·¦ï¼šåŸå›¾ï¼Œå³ï¼šç»†èƒæ ¸é¢„æµ‹æ©è†œ)",
        ])

        return "\n".join(report_lines)

    except Exception as e:
        return f"å›¾åƒåˆ†æå‡ºé”™: {type(e).__name__}: {str(e)}"